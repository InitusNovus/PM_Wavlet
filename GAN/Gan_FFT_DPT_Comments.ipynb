{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Callback\n",
    "import numpy as np\n",
    "# Setting the random seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Defining the batch size, available GPUs, and number of workers\n",
    "BATCH_SIZE=128\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "NUM_WORKERS=int(os.cpu_count() / 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code imports various libraries required for building a deep learning model using PyTorch. It also sets the random seed to 42 for reproducibility of results.\n",
    "\n",
    "Furthermore, it defines the batch size for the data loader, the number of available GPUs, and the number of workers for the data loader to use.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CustomDataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.FFT_files = os.listdir(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.FFT_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name=self.FFT_files[idx]\n",
    "        FFT_path = os.path.join(self.root_dir, file_name)\n",
    "        FFT = pd.read_csv(FFT_path,header=None).values\n",
    "        FFT_tensor = torch.tensor(FFT, dtype=torch.float32)\n",
    "        FFT_tensor = FFT_tensor.view(-1, FFT_tensor.shape[1])\n",
    "        Label_Tag=file_name.split('_')\n",
    "        label=[]\n",
    "        for n in [1,3,5,7,9,11,13]:\n",
    "            temp_num=float(Label_Tag[n])\n",
    "            label.append(temp_num)\n",
    "        label=torch.tensor(label)\n",
    "    \n",
    "        return FFT_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the 2D data matrix to a 3D matrix\n",
    "\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, csv_root, transform=None, batch_size=32, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.csv_root = csv_root   # Directory path where the CSV file is stored\n",
    "        self.batch_size = batch_size   # The batch size for the data loader\n",
    "        self.num_workers = num_workers   # The number of worker processes for loading the data\n",
    "        self.transform = transform   # Optional data transformation to be applied\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass   # Placeholder for any data preparation step, if needed\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = CustomDataset(self.csv_root, transform=self.transform)   # Initialize the CustomDataset class with the specified CSV directory and transform\n",
    "        #self.dataset_train, self.dataset_val = random_split(self.dataset,[int(len(self.dataset)*0.7),len(self.dataset)-int(len(self.dataset)*0.7)])\n",
    "        self.dataset_train = self.dataset   # Set the training dataset to be the entire dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)   # Return a DataLoader object for the training data, which shuffles the data and divides it into batches\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)   # Return a DataLoader object for the validation data, which is set to be the same as the training data\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return None   # No test data is used for this model, so return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the root directory of the dataset\n",
    "root_dir = '../Data/FFT_Data/Turn_off'\n",
    "\n",
    "# Defining the transformations for the dataset using PyTorch's Compose function\n",
    "# transformations = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# Creating a custom dataset and dataloader using the CustomDataModule class\n",
    "data_module = CustomDataModule(root_dir, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data module and data loader\n",
    "data_module.setup()\n",
    "\n",
    "# Retrieve the training data from the data loader\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "\n",
    "# Get the first batch of data from the data loader\n",
    "i, l = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 981])\n",
      "torch.Size([32, 7])\n"
     ]
    }
   ],
   "source": [
    "# Printing the shape of the input data and labels\n",
    "print(i.shape)\n",
    "print(l.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A discriminator model that determines if an image is real or fake, outputting a single value between 0 and 1\n",
    "\n",
    "## TODO: Change the Channel and input size\n",
    "\n",
    "# Start [batch, 3, 981] using 1-D Convolution, and using dialation for using data diffenrent part data \n",
    "# [batch, 3, 981] > [batch, 3, 979] > [batch, 3, 975] > [batch, 3, 967] > [batch, 3, 951] > [batch, 3, 919] > [batch, 3, 855] > [batch, 3, 727] > [batch, 3, 471] <[batch, 1, 469] \n",
    "# linear [469, 200, 1]\n",
    "#  \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # CNN architecture with dialation  references by TCN\n",
    "        self.conv1 = nn.Conv1d(3, 3, kernel_size=3,dilation=1)\n",
    "        self.conv2 = nn.Conv1d(3, 3, kernel_size=3,dilation=2)\n",
    "        self.conv3 = nn.Conv1d(3, 3, kernel_size=3,dilation=4)\n",
    "        self.conv4 = nn.Conv1d(3, 3, kernel_size=3,dilation=8)\n",
    "        self.conv5 = nn.Conv1d(3, 3, kernel_size=3,dilation=16)\n",
    "        self.conv6 = nn.Conv1d(3, 3, kernel_size=3,dilation=32)\n",
    "        self.conv7 = nn.Conv1d(3, 3, kernel_size=3,dilation=64)\n",
    "        self.conv8 = nn.Conv1d(3, 3, kernel_size=3,dilation=128)\n",
    "        self.conv9 = nn.Conv1d(3, 1, kernel_size=3)\n",
    "\n",
    "        # FCN\n",
    "        self.fc1 = nn.Linear(469, 200)\n",
    "        self.fc2 = nn.Linear(200, 1)\n",
    "\n",
    "        \n",
    "  \n",
    "    def forward(self, x):\n",
    "        # Apply convolutional and ReLU activation\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        # Flatten the tensor so it can be fed into the fully connected layers\n",
    "        temp_length=x.shape[2]\n",
    "        x = x.view(-1, temp_length)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply sigmoid activation to output a value between 0 and 1\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A generator model that takes a latent space vector as input and outputs a wavelet image\n",
    "\n",
    "## TODO: \n",
    "# Input_size [batch,7]\n",
    "# End_size [batch, 3, 981]\n",
    "\n",
    "# size history [7] > [64] > [488] [3,490] > [3, 981]\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the layers of the generator\n",
    "        self.lin1 = nn.Linear(7, 64)  \n",
    "        self.lin2 = nn.Linear(64, 488)  \n",
    "        self.ct1 = nn.ConvTranspose1d(1, 3, 3, stride=1) \n",
    "        self.ct2 = nn.ConvTranspose1d(3, 3, 3, stride=2) \n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through a linear layer and reshape\n",
    "        batch_size=x.shape[0]\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x=x.view(batch_size,1,-1)\n",
    "\n",
    "        # Upsample to  [batch, 3, 489]\n",
    "        x = self.ct1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Upsample to [batch,3, 981] (16 feature maps)\n",
    "        x = self.ct2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Change the input Data using the label\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "    def __init__(self,Result_path,lr=0.0002):\n",
    "        super().__init__()\n",
    "        # Save the hyperparameters and initialize the generator and discriminator\n",
    "        self.automatic_optimization=False\n",
    "        self.save_hyperparameters()\n",
    "        self.generator = Generator()\n",
    "        self.discriminator = Discriminator()\n",
    "        self.save_result_path=Result_path\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "    \n",
    "    def adverarial_loss(self, y_hat,y):\n",
    "        # Calculate binary cross-entropy loss\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        g_opt, d_opt = self.optimizers()\n",
    "        real_data, labels = batch\n",
    "        self.for_result=labels\n",
    "        # Sample noise\n",
    "        z = labels\n",
    "        z = z.type_as(real_data)\n",
    "        \n",
    "        # Train the generator: maximize log(D(G(z)))\n",
    "        generate_data = self(z)\n",
    "        y_hat = self.discriminator(generate_data)\n",
    "        \n",
    "        y = torch.ones(real_data.size(0), 1)\n",
    "        y = y.type_as(real_data)\n",
    "        \n",
    "        g_loss = self.adverarial_loss(y_hat, y)\n",
    "        \n",
    "        g_opt.zero_grad()\n",
    "        self.manual_backward(g_loss)\n",
    "        g_opt.step()\n",
    "        log_dict = {\"g_loss\" : g_loss }\n",
    "    \n",
    "        # Train the discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        y_hat_real = self.discriminator(real_data)\n",
    "        y_real = torch.ones(real_data.size(0), 1)\n",
    "        y_real = y_real.type_as(real_data)\n",
    "        real_loss = self.adverarial_loss(y_hat_real, y_real)\n",
    "        \n",
    "        y_hat_fake = self.discriminator(self(z))\n",
    "        y_fake = torch.zeros(real_data.size(0), 1)\n",
    "        y_fake = y_fake.type_as(real_data)\n",
    "        fake_loss = self.adverarial_loss(y_hat_fake, y_fake)\n",
    "        \n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        d_opt.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        d_opt.step()\n",
    "        log_dict = {\"d_loss\" : d_loss }\n",
    "        return {\"d_loss\": d_loss,\"g_loss\":g_loss, \"progress bar\" : log_dict, \"log\": log_dict}\n",
    "                \n",
    "    def configure_optimizers(self):\n",
    "        lr=self.hparams.lr\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr)\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr)\n",
    "        return opt_g, opt_d\n",
    "    \n",
    "    \n",
    "    def on_train_end(self):\n",
    "        z=self.for_result.type_as(self.generator.lin1.weight)\n",
    "        z=self(z)\n",
    "        Result=z.cpu().detach().numpy()\n",
    "        batch=Result.shape[0]\n",
    "        Result=Result.reshape(batch * 3 , -1)\n",
    "        Result= pd.DataFrame(Result,index=None)\n",
    "        Result.to_csv(self.save_result_path,header=False,index=False)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        print('epoch',self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Setting the root directory of the dataset\n",
    "root_dir = '../Data/FFT_Data/Turn_off'\n",
    "\n",
    "# Creating a custom dataset and dataloader using the CustomDataModule class\n",
    "data_module = CustomDataModule(root_dir, transform=None)\n",
    "# Create an instance of the GAN model\n",
    "save_result_path=\"../Data/Gan_Data/FFT/Result.csv\"\n",
    "model = GAN(Result_path=save_result_path)\n",
    "# Set up the trainer object\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMEET_SIMUL\\anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 32.3 K\n",
      "1 | discriminator | Discriminator | 94.5 K\n",
      "------------------------------------------------\n",
      "126 K     Trainable params\n",
      "0         Non-trainable params\n",
      "126 K     Total params\n",
      "0.507     Total estimated model params size (MB)\n",
      "c:\\Users\\SMEET_SIMUL\\anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\SMEET_SIMUL\\anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 15/15 [00:07<00:00,  1.90it/s, v_num=17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Epoch 0: 100%|██████████| 15/15 [00:07<00:00,  1.90it/s, v_num=17]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('forpytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e142c631c90029d590b66e5d4b447237bff44b182aad6284fd72863ce8560c1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
